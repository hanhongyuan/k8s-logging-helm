input {
{{- if .Values.logstash_indexer.beats }}
  
  beats {
    port => 5044
  }

{{ else }}  

  kafka {
    bootstrap_servers => ["${KAFKA_BOOTSTRAP_SERVERS}"]
    topics => ["containers"]
    client_id => "${HOSTNAME}-contaienrs"
    group_id => "containers"
    auto_offset_reset => "latest"
    consumer_threads => 10
  }  
{{- end }}

}

filter {
  #all messages from beats are in json format
  json {
    skip_on_invalid_json => true
    source => "message"
  }

  #all application json formated logs can be parsed
  #not for xman untill an agreed structure is in place
  # we parse the payload of the message field for json
  json {
      skip_on_invalid_json => true
      source => "message"
      add_tag => [ "json_parsed_successful" ]
  }

  mutate {
    add_field => { "id" => "%{[@metadata][_id]}" }
  }

  fingerprint {
     concatenate_sources => true
     source => ["@timestamp", "log.offset", "log.path.file"]
  }

}

output {

    elasticsearch {
      hosts => ["${ELASTICSEARCH_HOST}"]
      user => "${ELASTICSEARCH_USER:\"\"}"
      password => "${ELASTICSEARCH_PASSWORD:\"\"}"
      index => "containers-%{+YYYY.MM.dd}"
      manage_template => true
      template => "/usr/share/logstash/config/containers_template.json"
      template_name => "containers_1"
      template_overwrite => true
      ilm_enabled => "false"
    }
}